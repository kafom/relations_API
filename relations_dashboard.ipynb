{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:472: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\fabio\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:497: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 food 1 food\n",
      "2 cruise line 1 overall\n",
      "3 people 1 people\n",
      "4 msc 1 overall\n",
      "5 dining room 1 dining\n",
      "6 worst cruise 0 overall\n",
      "7 cruise 1 overall\n",
      "8 ship 1 ship\n",
      "9 cruise ship 1 ship\n",
      "10 time 1 time\n",
      "11 buffet 1 food\n",
      "12 room service 1 food\n",
      "13 board 1 ship\n",
      "14 poor quality 1 service\n",
      "15 msc cruise 1 overall\n",
      "16 customer service 1 service\n",
      "17 bar service 1 dining\n",
      "18 poor service 1 service\n",
      "19 passengers 1 people\n",
      "20 cruise lines 1 overall\n",
      "21 cruise director 1 service\n",
      "22 poor food 1 food\n",
      "23 public areas 1 ship\n",
      "24 credit card 0 None\n",
      "25 waiter 1 dining\n",
      "26 table 1 dining\n",
      "27 bar staff 1 dining\n",
      "28 times 1 time\n",
      "29 entertainment 1 entertainment\n",
      "30 boat 1 ship\n",
      "31 mistake 1 overall\n",
      "32 big mistake 1 overall\n",
      "33 negative reviews 0 overall\n",
      "34 biggest disappointment 0 overall\n",
      "35 disappointment 0 overall\n",
      "36 ice cream 1 food\n",
      "37 bad weather 0 overall\n",
      "38 food quality 1 food\n",
      "39 waiters 1 dining\n",
      "40 buffet restaurant 1 dining\n",
      "41 night 1 entertainment\n",
      "42 bad food 1 food\n",
      "43 main dining room 1 dining\n",
      "44 complete waste 0 None\n",
      "45 embarkation 1 service\n",
      "46 bad experience 0 None\n",
      "47 dining rooms 1 dining\n",
      "48 problem 0 None\n",
      "49 shore excursions 0 None\n",
      "50 money 0 None\n",
      "data/alchemy_ratings_relations_cleaned.csv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\fabio\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:519: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass DataFrame with boolean values only",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c664c7b40557>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-c664c7b40557>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprepare_relations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[0mtop_bad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_top_dataframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_keywords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcruiseLines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_keywords_in_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_bad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-c664c7b40557>\u001b[0m in \u001b[0;36mfind_keywords_in_text\u001b[1;34m(df, top_bad)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m     \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SbjSentType'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'neutral'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ObjSentType'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m \u001b[1;34m'neutral'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelete_useless_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m     \u001b[0msave_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"relations_small\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-c664c7b40557>\u001b[0m in \u001b[0;36mdelete_useless_rows\u001b[1;34m(mask, df)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdelete_useless_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\fabio\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1781\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1782\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1783\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1784\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_mi_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1785\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\fabio\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_frame\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1853\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1855\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Must pass DataFrame with boolean values only'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1856\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass DataFrame with boolean values only"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import math\n",
    "import csv\n",
    "from alchemyapi import AlchemyAPI\n",
    "\n",
    "def open_json_review_files(cruiseLines):\n",
    "    \"\"\"\n",
    "    creates dictionary with one or more cruise lines review json files\n",
    "    \"\"\"\n",
    "    lineDb= {}\n",
    "    commentDb = {}\n",
    "    totcount = 0\n",
    "    for line in cruiseLines:\n",
    "        count = 0\n",
    "        with open('data/'+line+'.json', 'rb') as fp:\n",
    "            lineDb[line] = json.load(fp)\n",
    "            commentDb.update(lineDb)\n",
    "            for element in commentDb[line]:\n",
    "                commentDb[line][element][\"line\"]=line\n",
    "                count = count + 1 \n",
    "        totcount=totcount+count\n",
    "        print \"processed\", line, len(commentDb[line]),\"reviews\"\n",
    "    print 'total processed', totcount, 'reviews'\n",
    "    return commentDb\n",
    "\n",
    "def cleanRatings(commentDb,cruiseLines):\n",
    "    \"\"\"\n",
    "    cleans up and aggregates ratings data\n",
    "    \"\"\"\n",
    "    ratingAggregations={\"6\":\"good\",\"5\":\"good\",\"4\":\"medium\",\"3\":\"medium\",\"2\":\"bad\",\"1\":\"bad\",\"no rating\":\"no rating\"}\n",
    "    count = 0 \n",
    "    for line in cruiseLines:\n",
    "        for element in commentDb[line]:\n",
    "            if len(commentDb[line][element][\"kind\"]) <5:\n",
    "                   commentDb[line][element][\"kind\"]=\"not specified\"\n",
    "            if not commentDb[line][element][\"rating\"].isdigit():\n",
    "                    commentDb[line][element][\"rating\"]=\"no rating\" \n",
    "            if len(commentDb[line][element][\"ship\"]) <5:\n",
    "                    commentDb[line][element][\"ship\"]=\"not available\"                     \n",
    "            commentDb[line][element][\"aggregatedRating\"]=ratingAggregations[commentDb[line][element][\"rating\"]] \n",
    "    return commentDb\n",
    "\n",
    "def checkDailyQuotaAndRunAlchemy(commentDb,cruiseLines,calls):\n",
    "    returned_data={}\n",
    "    alchemyapi = AlchemyAPI()\n",
    "    for call in calls:\n",
    "        fileName='data/Alchemy_response_'+call+'.json'\n",
    "        with open(fileName, 'rb') as fp:\n",
    "                returned_data[call] = json.load(fp)\n",
    "        test=\"test if finished Alchemy daily quota\"\n",
    "        response = alchemyapi.keywords('text', test, {'sentiment': 0})\n",
    "        if response['status'] == 'OK':\n",
    "            returned_data=runAlchemyApi(cruiseLines,commentDb,returned_data,call,alchemyapi)\n",
    "        else:\n",
    "            print 'Error in',call,' extraction call: ', response['statusInfo']\n",
    "    return returned_data\n",
    "\n",
    "def merge_old_and_new_alchemy_files(returned_data,calls,commentDb):\n",
    "    returned_data_old={}\n",
    "    elements_to_drop=[]\n",
    "    for call in calls:\n",
    "        for element in returned_data[call]:\n",
    "            if returned_data[call][element]['rating']=='no rating':\n",
    "                elements_to_drop.append(element)\n",
    "        for key in elements_to_drop:\n",
    "                returned_data[call].pop(key, None)       \n",
    "        fileName='data/Alchemy_response_'+call+'_old.json'\n",
    "        with open(fileName, 'rb') as fp:\n",
    "                returned_data_old[call] = json.load(fp)                               \n",
    "        for element in returned_data_old[call]:\n",
    "            if element not in returned_data[call]:\n",
    "                returned_data[call][element]=returned_data_old[call][element] \n",
    "    return returned_data\n",
    "\n",
    "def callAPI(text,alchemyapi,call,count,returned_data,commentDb,element,line,daily_alchemy_finished):\n",
    "    api_call={'keywords':alchemyapi.keywords('text', text, {'sentiment': 1}),\n",
    "             'relations':alchemyapi.relations('text', text, {'sentiment': 1, 'entities':1,'keywords':0}),\n",
    "              'entities':alchemyapi.entities('text', text, {'sentiment': 1,'knowledgeGraph':1}),\n",
    "              'concepts':alchemyapi.entities('text', text, {'showSourceText':1,'knowledgeGraph':1})\n",
    "             }\n",
    "    response = api_call[call]\n",
    "    if response['status'] == 'OK':\n",
    "        returned_data[call]=load_element_data(response,returned_data[call],commentDb,element,line)            \n",
    "        count=count+1       \n",
    "    else:\n",
    "        print 'Error in keyword extaction call: ', response['statusInfo']\n",
    "        daily_alchemy_finished=\"True\"\n",
    "    return returned_data, count,daily_alchemy_finished\n",
    "    \n",
    "\n",
    "def runAlchemyApi(cruiseLines, commentDb,returned_data,call,alchemyapi):\n",
    "    count=0\n",
    "    daily_alchemy_finished=False\n",
    "    check=0\n",
    "    low_ratings={\"1\",\"2\",\"5\",\"6\"}    \n",
    "    conta=0\n",
    "    for line in cruiseLines:\n",
    "        print 'getting Alchemy',call,'data. We already have keywords of',len(returned_data[call]),\"reviews for\", line\n",
    "        for element in commentDb[line]:\n",
    "            text=commentDb[line][element][\"comment\"]\n",
    "            if daily_alchemy_finished or element in returned_data or len(text) <100 or commentDb[line][element]['rating'] == \"no rating\":\n",
    "                pass\n",
    "            else:\n",
    "                if call!=\"keywords\" or commentDb[line][element]['rating'] in low_ratings:\n",
    "                    returned_data,count,daily_alchemy_finished=callAPI(text,alchemyapi,call,count,returned_data,commentDb,element,line,daily_alchemy_finished)\n",
    "    \n",
    "    \n",
    "    print 'finished getting',call,'data from Alchemy for', count,\"reviews\"        \n",
    "    print 'in total we have got',call,'data for', len(returned_data[call]),\"reviews\"\n",
    "    save_dictionary(returned_data[call],call) \n",
    "    return returned_data\n",
    "\n",
    "def load_element_data(response,returned_data,commentDb,element,line):   \n",
    "    returned_data[element]={}\n",
    "    returned_data[element]=response\n",
    "    returned_data[element][\"rating\"]=commentDb[line][element]['rating']\n",
    "    returned_data[element]['ship']=commentDb[line][element]['ship']\n",
    "    returned_data[element]['sail_date']=commentDb[line][element]['sail Date']\n",
    "    returned_data[element]['line']=line\n",
    "    return returned_data\n",
    "\n",
    "def save_dictionary(returned_dictionary,call):\n",
    "    with open('data/Alchemy_response_'+call+'.json', 'wb') as fp:\n",
    "        json.dump(returned_dictionary, fp) \n",
    "        \n",
    "def check_what_lines_we_are_processing(df):\n",
    "    series = df['Line'].value_counts()\n",
    "    print \"we are processing\",series.head(), '\\n'\n",
    "                     \n",
    "def make_keywords_csv_alchemy(returned_keywords,commentDb):\n",
    "    \"\"\"\n",
    "    generates the csv file that powers the keyword dashboard\n",
    "    \"\"\"\n",
    "    call=\"keywords\"\n",
    "    df=clean_dictionary_keys(returned_keywords,call,commentDb)\n",
    "    #check_what_lines_we_are_processing(df)    \n",
    "    patterns = [(r'[^A-Za-z0-9 ]+',''),(r' +',' ')]\n",
    "    regex_columns=[\"Word\"]\n",
    "    df=clean_with_regex(regex_columns,patterns,df)    \n",
    "    columns=[\"Word\"]\n",
    "    make_lowercase(columns,df)\n",
    "    mask=df.Word.str.len() >= 2\n",
    "    df=delete_useless_rows(mask,df)   \n",
    "    mask=df.Rating!=\"no rating\"\n",
    "    df=delete_useless_rows(mask,df)  \n",
    "    to_float=[\"relevance\",\"sentiment_score\",\"Rating\"]\n",
    "    df=convert_to_float(to_float, df)\n",
    "    minimum_scores={\"standard\":0.50,\"high\":0.6,\"very_high\":0.7}\n",
    "    for hypothesis in minimum_scores:\n",
    "        filter_data_and_generate_hypothesis(df,hypothesis,minimum_scores)        \n",
    "        \n",
    "\n",
    "def make_relations_csv_alchemy(returned_relations,commentDb):\n",
    "    \"\"\"\n",
    "    generates the csv file that powers the relations dashboard\n",
    "    \"\"\"\n",
    "    call=\"relations\"\n",
    "    df=clean_dictionary_keys(returned_relations,call,commentDb)\n",
    "    #check_what_lines_we_are_processing(df)\n",
    "    patterns = [(r'[^A-Za-z0-9%\\' ]+',''),(r' +',' ')]\n",
    "    text_columns=[\"sbjText\",\"actText\", \"objText\",\"location.text\"]\n",
    "    df=clean_with_regex(text_columns,patterns,df)\n",
    "    make_lowercase(text_columns,df)\n",
    "    to_float=[\"sbjSentScore\",\"objSentScore\",\"object.sentimentFromSubject.score\",\"location.sentiment.score\"]\n",
    "    df=convert_to_float(to_float, df)    \n",
    "    save_csv(df,call)   \n",
    "\n",
    "    \n",
    "    \n",
    "def make_entities_csv_alchemy(returned_relations,commentDb):\n",
    "    \"\"\"\n",
    "    generates the csv file that powers the relations dashboard\n",
    "    \"\"\"\n",
    "    call=\"entities\"\n",
    "    df=clean_dictionary_keys(returned_relations,call,commentDb)\n",
    "    patterns = [(r'[^A-Za-z0-9%\\' ]+',''),(r' +',' ')]\n",
    "    text_columns=[\"text\"]\n",
    "    to_drop=[\"disambiguated.census\",\"disambiguated.ciaFactbook\",\"disambiguated.crunchbase\",\"disambiguated.dbpedia\",\n",
    "             \"disambiguated.freebase\",\"disambiguated.geo\",\"disambiguated.geonames\",\"disambiguated.musicBrainz\",\n",
    "             \"disambiguated.name\",\"disambiguated.opencyc\",\"disambiguated.subType\",\"disambiguated.website\",\n",
    "             \"disambiguated.yago\",\"sentiment.mixed\",\"count\"]\n",
    "    df=drop_columns(to_drop,df)\n",
    "    mask=df[\"sentiment.type\"]!=\"neutral\"\n",
    "    df=delete_useless_rows(mask,df)  \n",
    "    all_types=[\"City\",\"Organization\",\"Continent\",\"Country\",\"HealthCondition\",\"Person\",\"Region\",\"FieldTerminology\",\n",
    "               \"TelevisionStation\",\"Company\",\"StateOrCounty\",\"Holiday\",\"Degree\",\"JobTitle\",\"Facility\",\"GeographicFeature\",\n",
    "               \"Movie\",\"NaturalDisaster\",\"Technology\",\"PrintMedia\",\"Drug\",\"Sport\",\n",
    "               \"OperatingSystem\",\"Product\",\"TelevisionShow\",\"EntertainmentAward\",\"MusicGroup\",\"ProfessionalDegree\"]\n",
    "    interesting_types=[\"Organization\",\"HealthCondition\",\"JobTitle\",\"ProfessionalDegree\"]\n",
    "    df=df[df[\"type\"].isin(interesting_types)]    \n",
    "    df=clean_with_regex(text_columns,patterns,df)\n",
    "    make_lowercase(text_columns,df)\n",
    "    to_float=[]\n",
    "    df=convert_to_float(to_float, df)    \n",
    "    save_csv(df,call)  \n",
    "    \n",
    "def make_concepts_csv_alchemy(returned_concepts,commentDb):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "def clean_dictionary_keys(returned_dictionary,call,commentDb):\n",
    "    df=pd.DataFrame()\n",
    "    count_keywords=0\n",
    "    count_relations=0\n",
    "    conta=0\n",
    "    keys_to_drop=[\"language\",\"status\",\"usage\",\"totalTransactions\",\"url\"]\n",
    "    to_rename=  {\"keywords\":{\"text\":\"Word\",'relevance_x': 'relevance','sentiment.score': 'sentiment_score',\n",
    "        \"sentiment\":\"dictionary\",'sentiment.type': 'Sentiment',\"rating\":\"Rating\",\"ship\":\"Ship\",\"line\":\"Line\"},\n",
    "        \"relations\":{\"action.lemmatized\":\"actLemma\",\n",
    "                     \"action.text\":\"actText\",\n",
    "                     \"action.verb.negated\":\"actVerbNeg\",\n",
    "                     \"action.verb.text\":\"actVerbText\",\n",
    "                     \"object.sentiment.score\":\"objSentScore\",\n",
    "                     \"object.sentiment.type\":\"objSentType\",\n",
    "                     \"object.text\":\"objText\",\n",
    "                     \"subject.sentiment.score\":\"sbjSentScore\",\n",
    "                     \"subject.sentiment.type\":\"sbjSentType\",\n",
    "                     \"subject.text\":\"sbjText\"           \n",
    "                     },\n",
    "          \"entities\":{}}\n",
    "    for review in returned_dictionary:\n",
    "        if returned_dictionary[review][\"language\"]!=\"english\":\n",
    "            print review, \"review seems not to be in English, but in\", returned_dictionary[\"language\"]\n",
    "        else:\n",
    "            for key in keys_to_drop:\n",
    "                returned_dictionary[review].pop(key, None)\n",
    "        if \"line\" in returned_dictionary[review]:        \n",
    "            line=returned_dictionary[review][\"line\"]                \n",
    "            df=flatten_dictionary(returned_dictionary,review,call,df,commentDb,line)\n",
    "    print df.info()\n",
    "    df.rename(columns=to_rename[call], inplace=True)\n",
    "    return df\n",
    "                \n",
    "        \n",
    "def clean_with_regex(regex_columns,patterns,df):\n",
    "    for column in regex_columns:\n",
    "        for pattern in patterns:\n",
    "            df[column].replace(pattern[0],pattern[1], regex = True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def filter_data_and_generate_hypothesis(df,hypothesis,minimum_scores):        \n",
    "    min_relevance=minimum_scores[hypothesis]\n",
    "    min_sentiment=minimum_scores[hypothesis]\n",
    "    mask=df.relevance>=min_relevance\n",
    "    df=delete_useless_rows(mask,df)\n",
    "    df=change_unsure_to_neutral(min_sentiment,df)    \n",
    "    mask=df.Sentiment!=\"neutral\"\n",
    "    df=delete_useless_rows(mask,df)\n",
    "    df=aggregate_ratings(df)\n",
    "    mask=df.Rating!=\"medium\"\n",
    "    df=delete_useless_rows(mask,df) \n",
    "    df=reformat_dates(df)\n",
    "    to_drop=[\"sentiment_score\",\"relevance\",\"sail_date\",\"date\",\"dictionary\",\"relevance_y\",\"sentiment.mixed\"]\n",
    "    df=drop_columns(to_drop,df)    \n",
    "    to_add={\"count\":1}\n",
    "    df=add_columns(to_add,df)\n",
    "    df=make_pivot(df)\n",
    "    df[\"Total\"]=df[\"Positive\"]+df[\"Negative\"]\n",
    "    to_rename={\"rating\":\"Rating\"}\n",
    "    df.rename(columns=to_rename, inplace=True)\n",
    "    save_csv(df,hypothesis)\n",
    "\n",
    "def flatten_dictionary(returned_dictionary,review,call,df,commentDb,line): \n",
    "    to_split=[\"location\",\"object\",\"subject\"]\n",
    "    if call == \"keywords\":\n",
    "        first_level=json_normalize(returned_dictionary[review],call,['rating','sail_date',\"ship\",\"line\"])\n",
    "        second_level=json_normalize(returned_dictionary[review][call])\n",
    "        together=pd.merge(first_level, second_level, on='text', how='outer')\n",
    "        df=pd.concat([df, together])\n",
    "    else:\n",
    "        if call == \"relations\":\n",
    "            for element in returned_dictionary[review][call]:\n",
    "                for my_column in to_split:\n",
    "                    if my_column in element:\n",
    "                        if \"entities\" in element[my_column]:\n",
    "                            first_name=my_column+'_ent_text'\n",
    "                            second_name=my_column+'_ent_type'\n",
    "                            element[first_name]=element[my_column][\"entities\"][0][\"text\"]\n",
    "                            element[second_name]=element[my_column][\"entities\"][0][\"type\"]\n",
    "                            del element[my_column]                  \n",
    "        if review in commentDb[line]:\n",
    "            rating=commentDb[line][review][\"rating\"]\n",
    "            for element in returned_dictionary[review][call]:\n",
    "                second_level=json_normalize(element)\n",
    "                second_level['review']=review\n",
    "                second_level['rating']=rating                \n",
    "                df=pd.concat([df,second_level])\n",
    "    return df \n",
    "\n",
    "\n",
    "def open_csv(suffix):\n",
    "    df= pd.DataFrame(pd.read_csv('data/alchemy_ratings_'+suffix+'.csv'))\n",
    "    return df\n",
    "\n",
    "def drop_columns(to_drop,df):    \n",
    "    for column in to_drop:\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def fill_na_columns(df,lista,fill_with):\n",
    "    for column in lista:\n",
    "        df[column].fillna(fill_with,inplace=True)\n",
    "        df[\"objSentScore\"].fillna(0,inplace=True)   \n",
    "    return df\n",
    "\n",
    "def reorder_column(df,new_list):\n",
    "    varlist =[w for w in df.columns if w not in new_list]\n",
    "    df = df[new_list+varlist]\n",
    "    return df \n",
    "\n",
    "\n",
    "def delete_useless_rows(mask,df):\n",
    "    df=df[mask]\n",
    "    return df\n",
    "\n",
    "def reformat_dates(df):\n",
    "    df[\"date\"]=pd.to_datetime(df.sail_date, format= \"%B %Y\", coerce= True)\n",
    "    df[\"Year\"]=pd.DatetimeIndex(df.date).year\n",
    "    return df\n",
    "\n",
    "def add_columns(to_add,df):                \n",
    "    for column in to_add:\n",
    "        df[column]=to_add[column]\n",
    "    return df\n",
    "\n",
    "def make_lowercase(columns,df):\n",
    "    for column in columns:\n",
    "        df[column]=df[column].str.lower()\n",
    "    return df\n",
    "\n",
    "def convert_to_float(to_float, df):\n",
    "    for column in to_float:\n",
    "        df[column]=df[column].astype(float).fillna(0.0)\n",
    "    return df\n",
    "\n",
    "def change_unsure_to_neutral(min_sentiment,df):\n",
    "    df[\"Sentiment\"][(df[\"sentiment_score\"] <= min_sentiment) & (df[\"sentiment_score\"] >= -min_sentiment)]=\"neutral\"\n",
    "    return df\n",
    "\n",
    "def make_pivot(df):\n",
    "    df=df.reset_index()  \n",
    "    dataDims=[\"Word\",\"Rating\",\"Year\",\"Ship\",\"Line\"]\n",
    "    valueDims=[\"count\"]\n",
    "    columnDims=[\"Sentiment\"]\n",
    "    labels=['Negative','Positive']\n",
    "    pivotValues=preparePivotValues(columnDims,df)   \n",
    "    df=pd.pivot_table(df,values=valueDims,index=dataDims,columns=columnDims,aggfunc='sum').fillna(0)\n",
    "    result={}\n",
    "    for valueDim in valueDims:\n",
    "        result[valueDim]= df[valueDim]\n",
    "        print 'processing ', valueDim, ' valuedimension'        \n",
    "        result[valueDim] =renamePivotColumns(result[valueDim],labels,pivotValues[0]) \n",
    "        result[valueDim]=result[valueDim].reset_index()\n",
    "        print 'processing pivot with ', len(columnDims) ,' pivot column dimensions and ', len(valueDims),' value column dimensions.....'        \n",
    "    if len(columnDims) == 1 and len(valueDims) == 1:\n",
    "        df=result[valueDim]          \n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def renamePivotColumns(result,labels,dimensions):\n",
    "    count=0    \n",
    "    print \"CHECK THAT RENAMING IS CORRECT ==>\" \n",
    "    for dimension in dimensions:\n",
    "        print \"renamed \", dimension, \" as \", labels[count]                \n",
    "        result.rename(columns={dimension:labels[count]}, inplace=True) \n",
    "        count=count+1\n",
    "    print \"\\n \"    \n",
    "    return result    \n",
    "    \n",
    "    \n",
    "def preparePivotValues(columnDims,df):\n",
    "    pivotValues={}\n",
    "    count = 0    \n",
    "    for columnDim in columnDims:\n",
    "        pivotValues[count]=df[columnDims[count]].unique()\n",
    "        pivotValues[count].sort()\n",
    "        if (len(pivotValues[count]) != 2):\n",
    "            print 'More than two pivot values in a column. POSSIBLE ERROR IN DATA ==> ', pivotValues[count] \n",
    "        count = count + 1\n",
    "    print \"these are the pivot values \",pivotValues\n",
    "    return pivotValues    \n",
    "\n",
    "\n",
    "def prepare_keywords(sum_field,rating,suffix,top_number,line):\n",
    "    df=open_csv(suffix)\n",
    "    mask=df.Line==line\n",
    "    df=delete_useless_rows(mask,df)\n",
    "    mask=df.Rating==rating\n",
    "    df=delete_useless_rows(mask,df)\n",
    "    df=get_top_words(df,sum_field,top_number)\n",
    "    return df\n",
    "\n",
    "def get_top_words(df,sum_field,top_number):\n",
    "    df = df.groupby('Word')\n",
    "    df=df[sum_field].sum().order(ascending=False)\n",
    "    df=df.head(top_number).copy()\n",
    "    return df   \n",
    "\n",
    "def save_csv(df,hypothesis):\n",
    "    fileName='data/alchemy_ratings_'+hypothesis+'.csv'\n",
    "    print fileName  \n",
    "    df.to_csv(fileName,encoding='utf-8')\n",
    "\n",
    "def blank_out_short_sentences(df,blank_out_rules):\n",
    "    for rule in blank_out_rules:\n",
    "        df.loc[df[rule[0]].str.len() <=rule[1], rule[0]] = \"\"\n",
    "    return df    \n",
    "\n",
    "def make_top_dataframes(top_number,line):\n",
    "    \"\"\"\n",
    "    spits out top keywords used in negative reviews\n",
    "    \"\"\"\n",
    "    suffix=\"standard\"\n",
    "    top_neg_bad=prepare_keywords(\"Negative\",\"bad\",suffix,top_number,line)\n",
    "    top_neg_bad=top_neg_bad.reset_index()\n",
    "    return top_neg_bad\n",
    "\n",
    "def prepare_relations():\n",
    "    \"\"\"\n",
    "    gets cvs output from API relations call and \n",
    "    cleans it up for dc.js\n",
    "    \"\"\"\n",
    "    suffix=\"relations\"\n",
    "    df=open_csv(suffix)\n",
    "    df=df.drop_duplicates()\n",
    "    #blank_out_rules=((\"objText\",3),(\"sbjText\",3))\n",
    "    #df=blank_out_short_sentences(df,blank_out_rules)    \n",
    "    to_rename={\"object.sentimentFromSubject.score\":\"objSentFromSbjScore\",\n",
    "               \"object.sentimentFromSubject.type\":\"objSentFromSbjType\",\n",
    "               \"action.verb.tense\":\"actVerbTense\",\n",
    "               \"location.sentiment.score\":\"locSentScore\",\n",
    "               \"location.sentiment.type\":\"locSentType\",\n",
    "               \"location.text\":\"locText\"\n",
    "               }\n",
    "    df.rename(columns=to_rename, inplace=True)\n",
    "    fill_na_str=[\"objText\",\"sbjText\",\"actVerbText\",\"locText\"]\n",
    "    fill_with=\"\"\n",
    "    df=fill_na_columns(df,fill_na_str,fill_with)\n",
    "    fill_na_int=[\"actVerbNeg\",\"objSentScore\",\"objSentFromSbjScore\",\"sbjSentScore\",\"locSentScore\"]\n",
    "    fill_with=0\n",
    "    df=fill_na_columns(df,fill_na_int,fill_with) \n",
    "    fill_na_sent=[\"objSentType\",\"objSentFromSbjType\",\"sbjSentType\",\"locSentType\"]\n",
    "    fill_with=\"neutral\"\n",
    "    df=fill_na_columns(df,fill_na_sent,fill_with)\n",
    "    to_rename={\"rating\":\"Rating\"}\n",
    "    df.rename(columns=to_rename, inplace=True)\n",
    "    df=aggregate_ratings(df)\n",
    "    to_rename={\"Rating\":\"rating\"}\n",
    "    df.rename(columns=to_rename, inplace=True)\n",
    "    new_order=[\"review\",\"rating\",\"sbjText\",\"actText\",\"actVerbNeg\",\"objText\",\"locText\",\"sbjSentType\",\n",
    "               \"objSentType\",\"objSentFromSbjType\",\"locSentType\",\"sbjSentScore\",\"objSentScore\",\n",
    "               \"objSentFromSbjScore\",\"actVerbText\",\"locSentScore\",\"actLemma\",\"actVerbTense\"]    \n",
    "    df = reorder_column(df,new_order)\n",
    "    to_drop=[\"Unnamed: 0\",\"actLemma\",\"actVerbText\",\"locSentScore\",\"actVerbTense\",\n",
    "             \"locSentType\",\"locText\",\"review\",\"objSentFromSbjScore\",\"objSentFromSbjType\"]\n",
    "    to_drop=[\"Unnamed: 0\"]\n",
    "    df=drop_columns(to_drop,df)\n",
    "    return df\n",
    "\n",
    "     \n",
    "def aggregate_ratings(df):\n",
    "    conditions=[((df[\"Rating\"] >= 5) & (df[\"Rating\"] <= 6),\"good\"),\n",
    "                ((df[\"Rating\"] >= 3) & (df[\"Rating\"] <= 4),\"medium\"),\n",
    "                ((df[\"Rating\"] >= 1) & (df[\"Rating\"] <= 2),\"bad\")]\n",
    "    for condition in conditions:\n",
    "        df[\"Rating\"][condition[0]]=condition[1]\n",
    "    return df \n",
    "\n",
    "def find_keywords_in_text(df, top_bad):\n",
    "    \"\"\"\n",
    "    Finds phrases that contains keywords and aggregates then according to similar issues. \n",
    "    ++> Must manually fill in 'interest_and_aggregation' dictionary with interesting and non interesting top keywords words.\n",
    "    \n",
    "    \"\"\"\n",
    "    count=0\n",
    "    with open('data/interest_and_aggregation.json', 'rb') as fp:\n",
    "            interest_and_aggregation = json.load(fp)\n",
    "    text_columns=[\"objText\",\"sbjText\",\"actVerbText\"]\n",
    "    for word in top_bad[\"Word\"]:\n",
    "        count=count + 1\n",
    "        if word in interest_and_aggregation:\n",
    "            print count, word, interest_and_aggregation[word][\"interesting\"], interest_and_aggregation[word][\"aggregation\"]   \n",
    "        else:\n",
    "            print \"===>\",count, word, 'not anywhere'      \n",
    "    for word in top_bad[\"Word\"]:\n",
    "        if interest_and_aggregation[word][\"interesting\"]==1:\n",
    "            for column in text_columns:\n",
    "                df.loc[df[column].str.contains(word), \"foundKeyword\"] = word\n",
    "                df.loc[df[column].str.contains(word), \"aggKeyword\"] = interest_and_aggregation[word][\"aggregation\"]\n",
    "    df=df[pd.notnull(df[\"foundKeyword\"])]\n",
    "    df[\"count\"]=1\n",
    "    save_csv(df,\"relations_cleaned\")\n",
    "    mask=df.actVerbNeg!=1\n",
    "    df=delete_useless_rows(mask,df)\n",
    "    mask=df.aggKeyword!=None\n",
    "    df=delete_useless_rows(mask,df)\n",
    "    df[\"message\"]=df[\"sbjText\"]+\" \"+df[\"actText\"]+\" \"+df[\"objText\"]+\" \"+df[\"locText\"]\n",
    "    df=df.reset_index()\n",
    "    to_drop=[\"review\",\"locText\",\"actVerbNeg\",\"actLemma\",\"actVerbTense\",\"actText\", \"sbjText\",\"objText\",\n",
    "             \"foundKeyword\",\"locSentType\",\"locSentScore\",\"objSentFromSbjType\",\"objSentFromSbjScore\"]\n",
    "    df=drop_columns(to_drop,df)\n",
    "    new_order=[\"rating\",\"aggKeyword\",\"message\",\"actVerbText\",\"sbjSentType\",\n",
    "               \"objSentType\",\"sbjSentScore\",\"objSentScore\",\"count\"]    \n",
    "    df = reorder_column(df,new_order)\n",
    "    to_rename={\"rating\":\"Rating\",\"aggKeyword\":\"AggKeyword\",\"message\":\"Message\",\"actVerbText\":\"ActVerbText\",\n",
    "                \"sbjSentType\":\"SbjSentType\",\"objSentType\":\"ObjSentType\",\"sbjSentScore\":\"SbjSentScore\",\n",
    "                \"objSentScore\":\"ObjSentScore\",\"count\":\"Count\"}\n",
    "    df.rename(columns=to_rename, inplace=True)\n",
    "    condition=[(df[\"SbjSentScore\"] >=-.6) & (df[\"SbjSentScore\"] <= .6),0]\n",
    "    df[\"SbjSentScore\"][condition[0]]=condition[1]\n",
    "    df.ix[df.SbjSentScore == 0, 'SbjSentType'] = \"neutral\"\n",
    "    condition=[(df[\"ObjSentScore\"] >=-.7) & (df[\"ObjSentScore\"] <= .7),0]\n",
    "    df[\"ObjSentScore\"][condition[0]]=condition[1]\n",
    "    df.ix[df.ObjSentScore == 0, 'ObjSentType'] = \"neutral\"\n",
    "    to_drop=[\"ObjSentScore\",\"SbjSentScore\",\"index\"]\n",
    "    df=drop_columns(to_drop,df)\n",
    "    df=df[(df['SbjSentType'] != 'neutral') | (df['ObjSentType']!= 'neutral')]\n",
    "    save_csv(df,\"relations_small\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def show_file(returned_data, number):\n",
    "    count = 0\n",
    "    for element in returned_data[\"relations\"]:\n",
    "        if count ==number:\n",
    "            my_json=returned_data[\"relations\"][element]\n",
    "            print json.dumps(my_json, indent=4, sort_keys=True)\n",
    "        count = count + 1\n",
    "\n",
    "\n",
    "\n",
    "cruiseLines=[\"Msc\",\"Costa\"]\n",
    "def main():\n",
    "    calls=['keywords','relations','entities','concepts']\n",
    "    calls=['relations']\n",
    "    #commentDb=open_json_review_files(cruiseLines)\n",
    "    #commentDb=cleanRatings(commentDb,cruiseLines)\n",
    "    #returned_data=checkDailyQuotaAndRunAlchemy(commentDb,cruiseLines,calls)\n",
    "    #returned_data=merge_old_and_new_alchemy_files(returned_data,calls,commentDb)\n",
    "    #number = 3\n",
    "    #show_file(returned_data,number)\n",
    "        \n",
    "    \n",
    "    #make_keywords_csv_alchemy(returned_data[\"keywords\"],commentDb)\n",
    "    #make_relations_csv_alchemy(returned_data[\"relations\"],commentDb)\n",
    "    #make_entities_csv_alchemy(returned_data[\"entities\"],commentDb)\n",
    "    #make_concepts_csv_alchemy(returned_data[\"concepts\"],commentDb)\n",
    "    top_keywords=50\n",
    "    df=prepare_relations()\n",
    "    top_bad=make_top_dataframes(top_keywords,cruiseLines[0])\n",
    "    df=find_keywords_in_text(df, top_bad)    \n",
    "    print df.info()\n",
    "    \n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
